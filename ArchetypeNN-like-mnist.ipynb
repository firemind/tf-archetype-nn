{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy\n",
    "import mynn as mynn\n",
    "import math\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "archetypes = []\n",
    "header = None\n",
    "cards = None\n",
    "classes = set()\n",
    "input_size = None\n",
    "with open('archetypes.csv', 'rb') as csvfile:\n",
    "  reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "  for row in reader:\n",
    "    if header == None:\n",
    "      header = row\n",
    "      cards = map(int, row[3:])\n",
    "      input_size = len(row)-3\n",
    "    elif row[2] == \"23\":\n",
    "      c = int(row[1])\n",
    "      classes.add(c)\n",
    "      rec = [map(float, row[3:]), c]\n",
    "      archetypes.append(rec)\n",
    "    else:\n",
    "      print row[2];\n",
    "      #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3958"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decks = []\n",
    "labels = []\n",
    "classes = list(classes)\n",
    "max_samples = 4000\n",
    "random.shuffle(archetypes)\n",
    "for rec in archetypes[:max_samples]:\n",
    "    decks.append(rec[0])\n",
    "    #labels.append( map(lambda x: (1.0 if x == rec[1] else 0.0), classes))\n",
    "    labels.append(classes.index(rec[1]))\n",
    "NUM_CLASSES = len(classes)\n",
    "len(decks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "\n",
    "  def __init__(self, decks, labels, dtype=tf.float32):\n",
    "    dtype = tf.as_dtype(dtype).base_dtype\n",
    "    if dtype not in (tf.uint8, tf.float32):\n",
    "      raise TypeError('Invalid image dtype %r, expected uint8 or float32' %\n",
    "                      dtype)\n",
    "\n",
    "\n",
    "    self._num_examples = len(decks)\n",
    "\n",
    "    # Convert shape from [num examples, rows, columns, depth]\n",
    "    # to [num examples, rows*columns] (assuming depth == 1)\n",
    "\n",
    "    self._decks = numpy.array(decks)\n",
    "    self._labels = numpy.array(labels)\n",
    "    self._epochs_completed = 0\n",
    "    self._index_in_epoch = 0\n",
    "\n",
    "  @property\n",
    "  def decks(self):\n",
    "    return self._decks\n",
    "\n",
    "  @property\n",
    "  def labels(self):\n",
    "    return self._labels\n",
    "\n",
    "  @property\n",
    "  def num_examples(self):\n",
    "    return self._num_examples\n",
    "\n",
    "  @property\n",
    "  def epochs_completed(self):\n",
    "    return self._epochs_completed\n",
    "\n",
    "  def next_batch(self, batch_size):\n",
    "    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "\n",
    "    start = self._index_in_epoch\n",
    "    self._index_in_epoch += batch_size\n",
    "    if self._index_in_epoch > self._num_examples:\n",
    "      # Finished epoch\n",
    "      self._epochs_completed += 1\n",
    "      # Shuffle the data\n",
    "      perm = numpy.arange(self._num_examples)\n",
    "      numpy.random.shuffle(perm)\n",
    "      print perm\n",
    "      self._decks = self._decks[perm]\n",
    "      self._labels = self._labels[perm]\n",
    "      # Start next epoch\n",
    "      start = 0\n",
    "      self._index_in_epoch = batch_size\n",
    "      assert batch_size <= self._num_examples\n",
    "    end = self._index_in_epoch\n",
    "    return self._decks[start:end], self._labels[start:end]\n",
    "\n",
    "split = int(len(decks) * 0.8)\n",
    "\n",
    "\n",
    "\n",
    "train = DataSet(decks[:split],     labels[:split])\n",
    "test  = DataSet(decks[(split):], labels[(split):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  3166\n",
      "Testing size:   792\n"
     ]
    }
   ],
   "source": [
    "print \"Training size: \", train.num_examples\n",
    "print \"Testing size:  \", test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def inference(images, hidden1_units, hidden2_units):\n",
    "  \"\"\"Build the MNIST model up to where it may be used for inference.\n",
    "  Args:\n",
    "    images: Images placeholder, from inputs().\n",
    "    hidden1_units: Size of the first hidden layer.\n",
    "    hidden2_units: Size of the second hidden layer.\n",
    "  Returns:\n",
    "    softmax_linear: Output tensor with the computed logits.\n",
    "  \"\"\"\n",
    "  # Hidden 1\n",
    "  with tf.name_scope('hidden1'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([input_size, hidden1_units],\n",
    "                            stddev=1.0 / math.sqrt(float(input_size))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                         name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "  # Hidden 2\n",
    "  with tf.name_scope('hidden2'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                         name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "  # Linear\n",
    "  with tf.name_scope('softmax_linear'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "                         name='biases')\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "  return logits\n",
    "\n",
    "\n",
    "def loss(logits, labels):\n",
    "  \"\"\"Calculates the loss from the logits and the labels.\n",
    "  Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size].\n",
    "  Returns:\n",
    "    loss: Loss tensor of type float.\n",
    "  \"\"\"\n",
    "  labels = tf.to_int64(labels)\n",
    "  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      logits, labels, name='xentropy')\n",
    "  loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "  return loss\n",
    "\n",
    "\n",
    "def training(loss, learning_rate):\n",
    "  \"\"\"Sets up the training Ops.\n",
    "  Creates a summarizer to track the loss over time in TensorBoard.\n",
    "  Creates an optimizer and applies the gradients to all trainable variables.\n",
    "  The Op returned by this function is what must be passed to the\n",
    "  `sess.run()` call to cause the model to train.\n",
    "  Args:\n",
    "    loss: Loss tensor, from loss().\n",
    "    learning_rate: The learning rate to use for gradient descent.\n",
    "  Returns:\n",
    "    train_op: The Op for training.\n",
    "  \"\"\"\n",
    "  # Add a scalar summary for the snapshot loss.\n",
    "  tf.scalar_summary(loss.op.name, loss)\n",
    "  # Create the gradient descent optimizer with the given learning rate.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  # Create a variable to track the global step.\n",
    "  global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "  # Use the optimizer to apply the gradients that minimize the loss\n",
    "  # (and also increment the global step counter) as a single training step.\n",
    "  train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "  return train_op\n",
    "\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "  \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "  Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "      range [0, NUM_CLASSES).\n",
    "  Returns:\n",
    "    A scalar int32 tensor with the number of examples (out of batch_size)\n",
    "    that were predicted correctly.\n",
    "  \"\"\"\n",
    "  # For a classifier model, we can use the in_top_k Op.\n",
    "  # It returns a bool tensor with shape [batch_size] that is true for\n",
    "  # the examples where the label is in the top k (here k=1)\n",
    "  # of all logits for that example.\n",
    "  correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "  # Return the number of true entries.\n",
    "  return tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "\n",
    "def fill_feed_dict(data_set, decks_pl, labels_pl):\n",
    "  \"\"\"Fills the feed_dict for training the given step.\n",
    "  A feed_dict takes the form of:\n",
    "  feed_dict = {\n",
    "      <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "      ....\n",
    "  }\n",
    "  Args:\n",
    "    data_set: The set of images and labels, from input_data.read_data_sets()\n",
    "    images_pl: The images placeholder, from placeholder_inputs().\n",
    "    labels_pl: The labels placeholder, from placeholder_inputs().\n",
    "  Returns:\n",
    "    feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "  \"\"\"\n",
    "  # Create the feed_dict for the placeholders filled with the next\n",
    "  # `batch size ` examples.\n",
    "  decks_feed, labels_feed = data_set.next_batch(batch_size)\n",
    "  feed_dict = {\n",
    "      decks_pl: decks_feed,\n",
    "      labels_pl: labels_feed,\n",
    "  }\n",
    "  return feed_dict\n",
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "  \"\"\"Runs one evaluation against the full epoch of data.\n",
    "  Args:\n",
    "    sess: The session in which the model has been trained.\n",
    "    eval_correct: The Tensor that returns the number of correct predictions.\n",
    "    images_placeholder: The images placeholder.\n",
    "    labels_placeholder: The labels placeholder.\n",
    "    data_set: The set of images and labels to evaluate, from\n",
    "      input_data.read_data_sets().\n",
    "  \"\"\"\n",
    "  # And run one epoch of eval.\n",
    "  true_count = 0  # Counts the number of correct predictions.\n",
    "  steps_per_epoch = data_set.num_examples // batch_size\n",
    "  num_examples = steps_per_epoch * batch_size\n",
    "  for step in xrange(steps_per_epoch):\n",
    "    feed_dict = fill_feed_dict(data_set,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "  precision = true_count / num_examples\n",
    "  print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 4.83 (0.098 sec)\n",
      "[1647 1777 1580 ..., 3048 1845 1207]\n",
      "[2707  548 1119 ..., 2960 2604  482]\n",
      "[2890 1231 2273 ..., 1260 2621 1775]\n",
      "[1896 1894  939 ...,  539  732 2018]\n",
      "[  16  372  773 ...,  237 1759 1368]\n",
      "Step 100: loss = 2.60 (0.004 sec)\n",
      "[2816  227  352 ...,   28  514 2865]\n",
      "[ 309 1493 2918 ..., 2504 2365  608]\n",
      "[1032 2327 1730 ...,  891 2402  373]\n",
      "[1001 2025 2728 ..., 2768 1109 2410]\n",
      "[ 315  985 2811 ...,  893  564 2184]\n",
      "Step 200: loss = 1.63 (0.004 sec)\n",
      "[ 517 2832  135 ..., 2357 1374 2031]\n",
      "[2361 1021 1079 ..., 2180 3079  696]\n",
      "[2147 1475  439 ..., 3159  723 1470]\n",
      "[1133 2279  674 ..., 2820 2743  695]\n",
      "[ 872   66 1398 ..., 1758  874  185]\n",
      "Step 300: loss = 1.00 (0.005 sec)\n",
      "[ 463 2266  414 ..., 2740 2261  285]\n",
      "[2222  890 2786 ..., 1593 2200 1631]\n",
      "[1376  523 2983 ..., 2996 2032  805]\n",
      "[1277  511  557 ...,  283 2892   47]\n",
      "[2131  428   41 ..., 1408 1818 3034]\n",
      "[1515 1376 1597 ...,  838  804 2648]\n",
      "Step 400: loss = 1.05 (0.004 sec)\n",
      "[ 434  151 2044 ..., 1563 1287 1352]\n",
      "[ 337   30  715 ..., 2043 1302 2492]\n",
      "[1774 1891 2888 ..., 1806  628 1685]\n",
      "[ 884  641 2310 ..., 2067  271 1441]\n",
      "[1125 1090  310 ..., 2780  777 1143]\n",
      "Step 500: loss = 0.60 (0.004 sec)\n",
      "[ 632 1891 1007 ...,  346 2813 2243]\n",
      "[ 742 1021 2772 ..., 3100  226 1363]\n",
      "[ 210 2847 2402 ...,  338 2759 1093]\n",
      "[2621 3118  359 ..., 2928 1998  537]\n",
      "[ 715  559 2748 ..., 1636 3087 2965]\n",
      "Step 600: loss = 0.48 (0.006 sec)\n",
      "[1419 1655  859 ..., 2194 2123 3078]\n",
      "[2186  775 2026 ..., 2751 2624 1681]\n",
      "[1986  138 2767 ..., 2285 1653 1809]\n",
      "[ 694 2686  890 ...,  945  404 1590]\n",
      "[ 636 1556 2478 ...,  919 1514 2544]\n",
      "Step 700: loss = 0.44 (0.005 sec)\n",
      "[ 469 1948 2652 ...,  298 1341 1776]\n",
      "[2420   67 1512 ..., 2341  279 2193]\n",
      "[2983 1851 2763 ..., 2016 1423   11]\n",
      "[1594 1253  232 ..., 2055 2231 2638]\n",
      "[1743 2383  347 ..., 2014   37  877]\n",
      "[3136 2709 1563 ..., 2274 2405 3096]\n",
      "Step 800: loss = 0.35 (0.006 sec)\n",
      "[1743 2067  886 ..., 1781  883 2556]\n",
      "[2441  514 1747 ...,  441 2945 1223]\n",
      "[ 925  246 1674 ...,  442 1559 1971]\n",
      "[2234 1133 2209 ..., 1330  192   31]\n",
      "[1720 2774 1745 ..., 2652  853 2906]\n",
      "Step 900: loss = 0.37 (0.004 sec)\n",
      "[ 438 1205 1546 ...,   70 1282 1191]\n",
      "[1829 1031 1475 ..., 1978 2652  872]\n",
      "[1658 1966 2214 ..., 2978 2708 1657]\n",
      "[  74 1804 1372 ..., 1872  741 2764]\n",
      "[1001 1893 2568 ..., 1999 1016  200]\n",
      "Training Data Eval:\n",
      "[ 360 1847  549 ..., 1776  297  934]\n",
      "  Num examples: 3040  Num correct: 2926  Precision @ 1: 0.0000\n",
      "Test Data Eval:\n",
      "  Num examples: 640  Num correct: 600  Precision @ 1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 160\n",
    "learning_rate = 0.1\n",
    "max_steps = 1000\n",
    "train_dir = './mytrain'\n",
    "with tf.Graph().as_default():\n",
    "    decks_placeholder = tf.placeholder(tf.float32, shape=(batch_size, input_size))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    logits = inference(decks_placeholder,\n",
    "                             64,\n",
    "                             16)\n",
    "    loss_op = loss(logits, labels_placeholder)\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    train_op = training(loss_op, learning_rate)\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "\n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.train.SummaryWriter(train_dir, sess.graph)\n",
    "\n",
    "    # And then after everything is built, start the training loop.\n",
    "    for step in xrange(max_steps):\n",
    "      start_time = time.time()\n",
    "\n",
    "      # Fill a feed dictionary with the actual set of images and labels\n",
    "      # for this particular training step.\n",
    "      feed_dict = fill_feed_dict(train,\n",
    "                                 decks_placeholder,\n",
    "                                 labels_placeholder)\n",
    "\n",
    "      # Run one step of the model.  The return values are the activations\n",
    "      # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "      # inspect the values of your Ops or variables, you may include them\n",
    "      # in the list passed to sess.run() and the value tensors will be\n",
    "      # returned in the tuple from the call.\n",
    "      _, loss_value = sess.run([train_op, loss_op],\n",
    "                               feed_dict=feed_dict)\n",
    "\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      # Write the summaries and print an overview fairly often.\n",
    "      if step % 100 == 0:\n",
    "        # Print status to stdout.\n",
    "        print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "        # Update the events file.\n",
    "        summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "        summary_writer.flush()\n",
    "\n",
    "      # Save a checkpoint and evaluate the model periodically.\n",
    "      if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "        #saver.save(sess, FLAGS.train_dir, global_step=step)\n",
    "        # Evaluate against the training set.\n",
    "        print('Training Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct,\n",
    "                decks_placeholder,\n",
    "                labels_placeholder,\n",
    "                train)\n",
    "        # Evaluate against the validation set.\n",
    "        #print('Validation Data Eval:')\n",
    "        #do_eval(sess,\n",
    "        #        eval_correct,\n",
    "        #        decks_placeholder,\n",
    "        #        labels_placeholder,\n",
    "        #        data_sets.validation)\n",
    "        # Evaluate against the test set.\n",
    "        print('Test Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct,\n",
    "                decks_placeholder,\n",
    "                labels_placeholder,\n",
    "                test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
